{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18151304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from numpy.random import randint\n",
    "from numpy import load, zeros, ones\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Concatenate, Activation\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import shutil\n",
    "shutil.copy(\"/content/drive/\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b50ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image_shape):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=image_shape))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(1, (4,4), padding='same', kernel_initializer=init))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    opt = Adam(lr=2e-5, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(image_shape = (256, 256, 3)):\n",
    "    init = RandomNormal(stddev=0.01)\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    e = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "    e = BatchNormalization()(e, training=True)\n",
    "    e3 = LeakyReLU(alpha=0.2)(e)\n",
    "\n",
    "    e = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(e3)\n",
    "    e = BatchNormalization()(e, training=True)\n",
    "    e2 = LeakyReLU(alpha=0.2)(e)\n",
    "    \n",
    "    e = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(e2)\n",
    "    e = BatchNormalization()(e, training=True)\n",
    "    e1 = LeakyReLU(alpha=0.2)(e)\n",
    "\n",
    "    for _ in range(3):\n",
    "        e = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(e1)\n",
    "        e = BatchNormalization()(e, training=True)\n",
    "        e = LeakyReLU(alpha=0.2)(e)\n",
    "\n",
    "        e = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(e)\n",
    "        e = BatchNormalization()(e, training=True)\n",
    "\n",
    "        e1 = Concatenate()([e, e1])\n",
    "\n",
    "    d = UpSampling2D((2, 2))(d1)\n",
    "    d = Conv2D(128, (1, 1), kernel_initializer=init)(d)\n",
    "    d = Dropout(0.5)(d, training=True)\n",
    "    d = Concatenate()([d, e2])\n",
    "    d = BatchNormalization()(d, training=True)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = UpSampling2D((2, 2))(d)\n",
    "    d = Conv2D(64, (1, 1), kernel_initializer=init)(d)\n",
    "    d = Dropout(0.5)(d, training=True)\n",
    "    d = Concatenate()([d, e3])\n",
    "    d = BatchNormalization()(d, training=True)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d, training=True)\n",
    "    out_im = Activation('tanh')(d)\n",
    "\n",
    "    model = Model(in_image, out_im)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator_model, discriminator_model, image_shape):\n",
    "    # Make weights in the discriminator not trainable\n",
    "    for layer in discriminator_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Define the source image\n",
    "    input_source_image = Input(shape=image_shape)\n",
    "\n",
    "    # Connect the source image to the generator input\n",
    "    generated_output = generator_model(input_source_image)\n",
    "\n",
    "    # Connect the source input and generator output to the discriminator input\n",
    "    discriminator_output = discriminator_model([input_source_image, generated_output])\n",
    "\n",
    "    # Source image as input, generated image, and classification output\n",
    "    gan_model = Model(input_source_image, [discriminator_output, generated_output])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    # Use binary cross-entropy for the discriminator and mean absolute error for the generator\n",
    "    gan_model.compile(loss=['binary_crossentropy', 'mae'], optimizer=optimizer, loss_weights=[1, 100])\n",
    "\n",
    "    return gan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d204e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(filename):\n",
    "    # load compressed arrays\n",
    "    data = load(filename)\n",
    "    # unpack arrays\n",
    "    M1, M2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    M1 = (M1 - 127.5) / 127.5\n",
    "    M2 = (M2 - 127.5) / 127.5\n",
    "    return [M2, M1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    trainA, trainB = dataset\n",
    "    x = randint(0, trainA.shape[0], n_samples)\n",
    "    M1, M2 = trainA[x], trainB[x]\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [M1, M2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    X = g_model.predict(samples)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(step, g_model, d_model, dataset, n_samples=3):\n",
    "    # save the generator model\n",
    "    filename2 = model_output + 'gen_model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    # save the discriminator model\n",
    "    filename3 = model_output + 'disc_model_%06d.h5' % (step+1)\n",
    "    d_model.save(filename3)\n",
    "    print('[.] Saved Step : %s' % (filename1))\n",
    "    print('[.] Saved Model: %s' % (filename2))\n",
    "    print('[.] Saved Model: %s' % (filename3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=12):\n",
    "    n_patch = d_model.output_shape[1]\n",
    "    trainA, trainB = dataset\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    print(\"[!] Number of steps {}\".format(n_steps))\n",
    "    print(\"[!] Saves model/step output at every {}\".format(bat_per_epo * 1))\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        start = time.time()\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "\n",
    "        time_taken = time.time() - start\n",
    "        print(\n",
    "            '[*] %06d, d1[%.3f] d2[%.3f] g[%06.3f] ---> time[%.2f], time_left[%.08s]'\n",
    "            %\n",
    "            (i + 1, d_loss1, d_loss2, g_loss, time_taken, str(datetime.timedelta(seconds=((time_taken) * (n_steps - (i + 1))))).split('.')[0].zfill(8))\n",
    "        )\n",
    "\n",
    "        if (i + 1) % (bat_per_epo * 1) == 0:\n",
    "            save_model(i, g_model, d_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_real_samples('/MyDrive/img.npz')\n",
    "image_shape = dataset[0].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = discriminator(image_shape)\n",
    "g_model = generator(image_shape)\n",
    "gan_model = define_gan(g_model, d_model, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/MyDrive/'\n",
    "fileName = 'Enhancement Model'\n",
    "step_output = dir + fileName + \"/Step Output/\"\n",
    "model_output = dir + fileName + \"/Model Output/\"\n",
    "if fileName not in os.listdir(dir):\n",
    "    os.mkdir(dir + fileName)\n",
    "    os.mkdir(step_output)\n",
    "    os.mkdir(model_output)\n",
    "\n",
    "train(d_model, g_model, gan_model, dataset, batch=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
